{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('All_cities_cleaned_data.csv', low_memory= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 552269 entries, 0 to 552268\n",
      "Data columns (total 22 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   ft                   552269 non-null  object \n",
      " 1   bt                   552269 non-null  object \n",
      " 2   km                   552269 non-null  int64  \n",
      " 3   transmission         552269 non-null  object \n",
      " 4   oem                  552269 non-null  object \n",
      " 5   model                552269 non-null  object \n",
      " 6   modelYear            552269 non-null  float64\n",
      " 7   price                552269 non-null  float64\n",
      " 8   Fuel Type            552269 non-null  object \n",
      " 9   Seats                552269 non-null  int64  \n",
      " 10  Kms Driven           552269 non-null  int64  \n",
      " 11  Ownership            552269 non-null  object \n",
      " 12  Year of Manufacture  552269 non-null  float64\n",
      " 13  Mileage              552269 non-null  float64\n",
      " 14  Engine               552269 non-null  int64  \n",
      " 15  Color                552269 non-null  object \n",
      " 16  Displacement         552269 non-null  int64  \n",
      " 17  Length               552269 non-null  int64  \n",
      " 18  Width                552269 non-null  int64  \n",
      " 19  Height               552269 non-null  int64  \n",
      " 20  Wheel Base           552269 non-null  int64  \n",
      " 21  City                 552269 non-null  object \n",
      "dtypes: float64(4), int64(9), object(9)\n",
      "memory usage: 92.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {\n",
    "    'ft': 'Fuel type', \n",
    "    'bt': 'Body type', \n",
    "    'km': 'Kilometers driven', \n",
    "    'oem': 'Original Equipment Manufacturer'}, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 552269 entries, 0 to 552268\n",
      "Data columns (total 22 columns):\n",
      " #   Column                           Non-Null Count   Dtype  \n",
      "---  ------                           --------------   -----  \n",
      " 0   Fuel type                        552269 non-null  object \n",
      " 1   Body type                        552269 non-null  object \n",
      " 2   Kilometers driven                552269 non-null  int64  \n",
      " 3   transmission                     552269 non-null  object \n",
      " 4   Original Equipment Manufacturer  552269 non-null  object \n",
      " 5   model                            552269 non-null  object \n",
      " 6   modelYear                        552269 non-null  float64\n",
      " 7   price                            552269 non-null  float64\n",
      " 8   Fuel Type                        552269 non-null  object \n",
      " 9   Seats                            552269 non-null  int64  \n",
      " 10  Kms Driven                       552269 non-null  int64  \n",
      " 11  Ownership                        552269 non-null  object \n",
      " 12  Year of Manufacture              552269 non-null  float64\n",
      " 13  Mileage                          552269 non-null  float64\n",
      " 14  Engine                           552269 non-null  int64  \n",
      " 15  Color                            552269 non-null  object \n",
      " 16  Displacement                     552269 non-null  int64  \n",
      " 17  Length                           552269 non-null  int64  \n",
      " 18  Width                            552269 non-null  int64  \n",
      " 19  Height                           552269 non-null  int64  \n",
      " 20  Wheel Base                       552269 non-null  int64  \n",
      " 21  City                             552269 non-null  object \n",
      "dtypes: float64(4), int64(9), object(9)\n",
      "memory usage: 92.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552269\n",
      "8258\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(len(df_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a CSV file\n",
    "\n",
    "df_unique.to_csv('Final_data1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature lists\n",
    "numeric_features = ['Kilometers driven', 'modelYear', 'price', 'Seats', 'Kms Driven', 'Year of Manufacture', 'Mileage', 'Engine', 'Displacement', 'Length', 'Width', 'Height', 'Wheel Base']\n",
    "categorical_features = ['Fuel type', 'Body type', 'transmission', 'Original Equipment Manufacturer', 'model', 'Fuel Type', 'Ownership', 'Color', 'City']\n",
    "\n",
    "# Create DataFrames for numeric and categorical features\n",
    "df_numeric = df_unique[numeric_features]\n",
    "df_categorical = df_unique[categorical_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siva\\AppData\\Local\\Temp\\ipykernel_35308\\4294831733.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_categorical['Bought'] = repeated_pattern\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the pattern\n",
    "pattern = np.array([1, 0, 1, 0, 1, 0])\n",
    "\n",
    "# Create the repeating pattern array\n",
    "repeated_pattern = np.tile(pattern, len(df_categorical) // len(pattern))\n",
    "remaining_length = len(df_categorical) % len(pattern)\n",
    "if remaining_length > 0:\n",
    "    repeated_pattern = np.concatenate([repeated_pattern, pattern[:remaining_length]])\n",
    "\n",
    "# Adding the new column\n",
    "df_categorical['Bought'] = repeated_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regressor with Numeric Features:\n",
      "\n",
      "Mean Absolute Error (MAE): 476132.1350134356\n",
      "Mean Squared Error (MSE): 557106151070.4108\n",
      "R-squared: 0.6111832981721214\n",
      "\n",
      "Best parameters for Ridge: {'regressor__alpha': 1.0}\n",
      "\n",
      "Ridge Regression with Numeric Features:\n",
      "\n",
      "Mean Absolute Error (MAE): 472555.2507111026\n",
      "Mean Squared Error (MSE): 553865022468.844\n",
      "R-squared: 0.61344535349253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.450e+15, tolerance: 1.211e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.714e+15, tolerance: 1.039e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.296e+15, tolerance: 1.186e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.784e+15, tolerance: 1.084e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.268e+15, tolerance: 1.189e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.375e+15, tolerance: 1.211e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.661e+15, tolerance: 1.039e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.206e+15, tolerance: 1.186e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+15, tolerance: 1.084e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.996e+15, tolerance: 1.189e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.616e+15, tolerance: 1.211e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.122e+15, tolerance: 1.039e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+15, tolerance: 1.186e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.095e+15, tolerance: 1.084e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.494e+14, tolerance: 1.189e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Siva\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.894e+15, tolerance: 1.427e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso Regression with Numeric Features:\n",
      "\n",
      "Mean Absolute Error (MAE): 472636.97735188075\n",
      "Mean Squared Error (MSE): 553995851588.613\n",
      "R-squared: 0.6133540449568875\n",
      "\n",
      "Elastic Net Regression with Numeric Features:\n",
      "\n",
      "Mean Absolute Error (MAE): 468238.4119907266\n",
      "Mean Squared Error (MSE): 548249536400.45245\n",
      "R-squared: 0.6173645253919557\n",
      "\n",
      "Random Forest Regressor with Numeric Features:\n",
      "\n",
      "Mean Absolute Error (MAE): 149440.06698484372\n",
      "Mean Squared Error (MSE): 178678298404.85098\n",
      "R-squared: 0.8752964645238477\n",
      "\n",
      "Decision Tree Regressor with Numeric Features:\n",
      "\n",
      "Mean Absolute Error (MAE): 189696.5433817595\n",
      "Mean Squared Error (MSE): 253417100544.32922\n",
      "R-squared: 0.8231346018508118\n",
      "\n",
      "Gradient Boosting Regressor with Numeric Features:\n",
      "\n",
      "Mean Absolute Error (MAE): 194069.68592407947\n",
      "Mean Squared Error (MSE): 298679420765.5149\n",
      "R-squared: 0.7915450277065219\n",
      "\n",
      "XGBoost Regressor with Numeric Features:\n",
      "\n",
      "Mean Absolute Error (MAE): 157108.52823282385\n",
      "Mean Squared Error (MSE): 219277440590.89648\n",
      "R-squared: 0.8469614254446898\n",
      "\n",
      "Best Parameters for KNN: {'knn__n_neighbors': 5, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "\n",
      "\n",
      "KNN Classifier with Hyperparameter Tuning Evaluation:\n",
      "\n",
      "Accuracy: 0.8704600484261501\n",
      "Precision: 0.87046721751791\n",
      "Recall: 0.8704600484261501\n",
      "F1 Score: 0.8704585294749986\n",
      "Confusion Matrix:\n",
      " [[723 105]\n",
      " [109 715]]\n",
      "\n",
      "Random Forest Classifier with Categorical Features:\n",
      "\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Confusion Matrix:\n",
      " [[828   0]\n",
      " [  0 824]]\n",
      "\n",
      "Gradient Boosting Classifier with Categorical Features:\n",
      "\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Confusion Matrix:\n",
      " [[828   0]\n",
      " [  0 824]]\n",
      "\n",
      "XGBoost Classifier with Categorical Features:\n",
      "\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Confusion Matrix:\n",
      " [[828   0]\n",
      " [  0 824]]\n",
      "\n",
      "CatBoost Classifier with Categorical Features:\n",
      "\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Confusion Matrix:\n",
      " [[828   0]\n",
      " [  0 824]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "# ----------------------- Regression Models -----------------------\n",
    "\n",
    "# Define features and target variable for regression\n",
    "X = df_numeric.drop('price', axis=1)  # Features\n",
    "y = df_numeric['price']               # Target variable\n",
    "\n",
    "# Define the numerical features\n",
    "numerical_features = [\n",
    "    'Kilometers driven', 'modelYear', 'Seats', 'Kms Driven', 'Year of Manufacture', \n",
    "    'Mileage', 'Engine', 'Displacement', 'Length', 'Width', 'Height', 'Wheel Base'\n",
    "]\n",
    "\n",
    "# Filter X to keep only the numerical features\n",
    "X = X[numerical_features]\n",
    "\n",
    "# Create a preprocessing pipeline for numerical data\n",
    "preprocessor = StandardScaler()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression\n",
    "pipeline_lr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "y_pred_lr = pipeline_lr.predict(X_test)\n",
    "print()  # Blank line for spacing\n",
    "print(\"Linear Regressor with Numeric Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_lr)}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred_lr)}\")\n",
    "print(f\"R-squared: {r2_score(y_test, y_pred_lr)}\")\n",
    "\n",
    "# Ridge Regression\n",
    "# Define the parameter grid for Ridge\n",
    "param_grid_ridge = {'regressor__alpha': [1.0]}\n",
    "\n",
    "# Set up the pipeline with Ridge regression\n",
    "pipeline_ridge = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(solver='auto', max_iter=10))  # Increase max_iter to allow for convergence\n",
    "])\n",
    "\n",
    "# Perform Grid Search with 20-fold cross-validation\n",
    "grid_search_ridge = GridSearchCV(pipeline_ridge, param_grid_ridge, cv=20, n_jobs=-1)\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters\n",
    "print()  # Blank line for spacing\n",
    "print(\"Best parameters for Ridge:\", grid_search_ridge.best_params_)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ridge = grid_search_ridge.best_estimator_.predict(X_test)\n",
    "\n",
    "# Output results\n",
    "print()  # Blank line for spacing\n",
    "print(\"Ridge Regression with Numeric Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_ridge)}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred_ridge)}\")\n",
    "print(f\"R-squared: {r2_score(y_test, y_pred_ridge)}\")\n",
    "\n",
    "\n",
    "# Lasso Regression\n",
    "# Define the parameter grid for Lasso\n",
    "param_grid_lasso = {'regressor__alpha': [0.1, 1, 10]}\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline_lasso = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Lasso())\n",
    "])\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_lasso = GridSearchCV(estimator=pipeline_lasso, param_grid=param_grid_lasso, cv=5)\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lasso = grid_search_lasso.best_estimator_.predict(X_test)\n",
    "\n",
    "# Output results\n",
    "print()  # Blank line for spacing\n",
    "print(\"Lasso Regression with Numeric Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_lasso)}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred_lasso)}\")\n",
    "print(f\"R-squared: {r2_score(y_test, y_pred_lasso)}\")\n",
    "\n",
    "\n",
    "# Elastic Net Regression\n",
    "# Define the parameter grid for Elastic Net\n",
    "param_grid_en = {\n",
    "    'regressor__alpha': [0.1, 1, 10],\n",
    "    'regressor__l1_ratio': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline_en = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', ElasticNet())\n",
    "])\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_en = GridSearchCV(estimator=pipeline_en, param_grid=param_grid_en, cv=5)\n",
    "grid_search_en.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_elastic_net = grid_search_en.best_estimator_.predict(X_test)\n",
    "\n",
    "# Output results\n",
    "print()  # Blank line for spacing\n",
    "print(\"Elastic Net Regression with Numeric Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_elastic_net)}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred_elastic_net)}\")\n",
    "print(f\"R-squared: {r2_score(y_test, y_pred_elastic_net)}\")\n",
    "\n",
    "\n",
    "# Random Forest Regressor\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "y_pred_rf = pipeline_rf.predict(X_test)\n",
    "print()  # Blank line for spacing\n",
    "print(\"Random Forest Regressor with Numeric Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_rf)}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "print(f\"R-squared: {r2_score(y_test, y_pred_rf)}\")\n",
    "\n",
    "# Decision Tree Regressor\n",
    "pipeline_dt = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "pipeline_dt.fit(X_train, y_train)\n",
    "y_pred_dt = pipeline_dt.predict(X_test)\n",
    "print()  # Blank line for spacing\n",
    "print(\"Decision Tree Regressor with Numeric Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_dt)}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred_dt)}\")\n",
    "print(f\"R-squared: {r2_score(y_test, y_pred_dt)}\")\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "pipeline_gb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "pipeline_gb.fit(X_train, y_train)\n",
    "y_pred_gb = pipeline_gb.predict(X_test)\n",
    "print()  # Blank line for spacing\n",
    "print(\"Gradient Boosting Regressor with Numeric Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_gb)}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred_gb)}\")\n",
    "print(f\"R-squared: {r2_score(y_test, y_pred_gb)}\")\n",
    "\n",
    "# XGBoost Regressor\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', xgb.XGBRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "pipeline_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = pipeline_xgb.predict(X_test)\n",
    "print()  # Blank line for spacing\n",
    "print(\"XGBoost Regressor with Numeric Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_xgb)}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred_xgb)}\")\n",
    "print(f\"R-squared: {r2_score(y_test, y_pred_xgb)}\")\n",
    "\n",
    "# ----------------------- Classification Models -----------------------\n",
    "\n",
    "# Define categorical features and target variable for classification\n",
    "categorical_features = [\n",
    "    'Fuel type', 'Body type', 'transmission', 'Original Equipment Manufacturer',\n",
    "    'model', 'Fuel Type', 'Ownership', 'Color', 'City', 'Bought'\n",
    "]\n",
    "\n",
    "# Extract features and target variable for classification\n",
    "X_categorical = df_categorical[categorical_features]\n",
    "y_categorical = df_categorical['Bought']\n",
    "\n",
    "# Create a preprocessing pipeline for categorical data\n",
    "preprocessor_cat = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Use dense array\n",
    "        ]), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_categorical, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# KNN Classifier\n",
    "pipeline_knn = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_cat),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "param_grid_knn = {\n",
    "    'knn__n_neighbors': [5],\n",
    "    'knn__weights': ['distance'],\n",
    "    'knn__p': [1]\n",
    "}\n",
    "grid_search_knn = GridSearchCV(pipeline_knn, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_knn.fit(X_train_cat, y_train_cat)\n",
    "print()  # Blank line for spacing\n",
    "print(\"Best Parameters for KNN:\", grid_search_knn.best_params_)\n",
    "print()  # Blank line for spacing\n",
    "y_pred_knn = grid_search_knn.best_estimator_.predict(X_test_cat)\n",
    "print()  # Blank line for spacing\n",
    "print(\"KNN Classifier with Hyperparameter Tuning Evaluation:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Accuracy: {accuracy_score(y_test_cat, y_pred_knn)}\")\n",
    "print(f\"Precision: {precision_score(y_test_cat, y_pred_knn, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test_cat, y_pred_knn, average='weighted')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_cat, y_pred_knn, average='weighted')}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_cat, y_pred_knn))\n",
    "\n",
    "# Random Forest Classifier\n",
    "pipeline_rf_clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_cat),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "pipeline_rf_clf.fit(X_train_cat, y_train_cat)\n",
    "y_pred_rf_clf = pipeline_rf_clf.predict(X_test_cat)\n",
    "print()  # Blank line for spacing\n",
    "print(\"Random Forest Classifier with Categorical Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Accuracy: {accuracy_score(y_test_cat, y_pred_rf_clf)}\")\n",
    "print(f\"Precision: {precision_score(y_test_cat, y_pred_rf_clf, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test_cat, y_pred_rf_clf, average='weighted')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_cat, y_pred_rf_clf, average='weighted')}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_cat, y_pred_rf_clf))\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "pipeline_gb_clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_cat),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "pipeline_gb_clf.fit(X_train_cat, y_train_cat)\n",
    "y_pred_gb_clf = pipeline_gb_clf.predict(X_test_cat)\n",
    "print()  # Blank line for spacing\n",
    "print(\"Gradient Boosting Classifier with Categorical Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Accuracy: {accuracy_score(y_test_cat, y_pred_gb_clf)}\")\n",
    "print(f\"Precision: {precision_score(y_test_cat, y_pred_gb_clf, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test_cat, y_pred_gb_clf, average='weighted')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_cat, y_pred_gb_clf, average='weighted')}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_cat, y_pred_gb_clf))\n",
    "\n",
    "# XGBoost Classifier\n",
    "pipeline_xgb_clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_cat),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', XGBClassifier(eval_metric='mlogloss'))\n",
    "])\n",
    "pipeline_xgb_clf.fit(X_train_cat, y_train_cat)\n",
    "y_pred_xgb_clf = pipeline_xgb_clf.predict(X_test_cat)\n",
    "print()  # Blank line for spacing\n",
    "print(\"XGBoost Classifier with Categorical Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Accuracy: {accuracy_score(y_test_cat, y_pred_xgb_clf)}\")\n",
    "print(f\"Precision: {precision_score(y_test_cat, y_pred_xgb_clf, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test_cat, y_pred_xgb_clf, average='weighted')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_cat, y_pred_xgb_clf, average='weighted')}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_cat, y_pred_xgb_clf))\n",
    "\n",
    "# CatBoost Classifier\n",
    "pipeline_catboost_clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_cat),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', CatBoostClassifier(silent=True))\n",
    "])\n",
    "pipeline_catboost_clf.fit(X_train_cat, y_train_cat)\n",
    "y_pred_catboost_clf = pipeline_catboost_clf.predict(X_test_cat)\n",
    "print()  # Blank line for spacing\n",
    "print(\"CatBoost Classifier with Categorical Features:\")\n",
    "print()  # Blank line for spacing\n",
    "print(f\"Accuracy: {accuracy_score(y_test_cat, y_pred_catboost_clf)}\")\n",
    "print(f\"Precision: {precision_score(y_test_cat, y_pred_catboost_clf, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test_cat, y_pred_catboost_clf, average='weighted')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_cat, y_pred_catboost_clf, average='weighted')}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_cat, y_pred_catboost_clf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 475480.9426774719\n",
      "Mean Squared Error (MSE): 556563938636.6627\n",
      "R-squared: 0.611561720219293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target variable for regression\n",
    "X = df_numeric.drop('price', axis=1)  # Features\n",
    "y = df_numeric['price']               # Target variable\n",
    "\n",
    "# Define the numerical features\n",
    "numerical_features = [\n",
    "    'Kilometers driven', 'modelYear', 'Seats', 'Kms Driven', 'Year of Manufacture', \n",
    "    'Mileage', 'Engine', 'Displacement', 'Length', 'Width', 'Height', 'Wheel Base'\n",
    "]\n",
    "\n",
    "# Filter X to keep only the numerical features\n",
    "X = X[numerical_features]\n",
    "\n",
    "# Feature selection with RFE\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=12)\n",
    "X_reduced = rfe.fit_transform(X, y)\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create the full pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"R-squared: {r2_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Assuming you already have `X_train` and `X_test` defined\n",
    "\n",
    "# Fit the preprocessor on the training data\n",
    "preprocessor = StandardScaler()\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Save the preprocessor\n",
    "joblib.dump(preprocessor, 'preprocessor_Reg.pkl')\n",
    "\n",
    "# Transform the test data\n",
    "X_test_transformed = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_pipeline_reg.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Load the preprocessor\n",
    "preprocessor = joblib.load('preprocessor_Reg.pkl')\n",
    "\n",
    "# Apply the preprocessor to the training data\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "\n",
    "# Create a pipeline with the preprocessor and RandomForestRegressor\n",
    "model_pipeline_reg = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the transformed training data\n",
    "model_pipeline_reg.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Save the entire pipeline including the model\n",
    "joblib.dump(model_pipeline_reg, 'model_pipeline_reg.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize continuous target into bins\n",
    "y_train_binned = pd.qcut(y_train, q=5, labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_pipeline_cls.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the preprocessor on training data and save it\n",
    "preprocessor.fit(X_train)\n",
    "joblib.dump(preprocessor, 'preprocessor_Cls.pkl')\n",
    "\n",
    "# Later, when you want to use this preprocessor with RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the preprocessor\n",
    "preprocessor = joblib.load('preprocessor_Cls.pkl')\n",
    "\n",
    "# Create a pipeline with the preprocessor and RandomForestClassifier\n",
    "model_pipeline_cls = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on training data\n",
    "model_pipeline_cls.fit(X_train, y_train_binned)\n",
    "\n",
    "# Save the entire pipeline including the model\n",
    "joblib.dump(model_pipeline_cls, 'model_pipeline_cls.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
